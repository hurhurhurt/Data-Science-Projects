{"cells":[{"cell_type":"code","source":["# Read in data\ndf = spark.read.format('csv') \\\n  .option(\"inferSchema\", 'true') \\\n  .option(\"header\", 'true') \\\n  .option(\"sep\", \",\") \\\n  .load(\"/FileStore/tables/Use_Case_6__India_s_Audt-77a21.csv\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["# Exploratory Data Analysis\n\nLet's run some basic tests to see what kind of data we're dealing with."],"metadata":{}},{"cell_type":"code","source":["# What kind of data are we working with?\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Sector_score: double (nullable = true)\n-- LOCATION_ID: string (nullable = true)\n-- PARA_A: double (nullable = true)\n-- Score_A: double (nullable = true)\n-- Risk_A: double (nullable = true)\n-- PARA_B: double (nullable = true)\n-- Score_B6: double (nullable = true)\n-- Risk_B: double (nullable = true)\n-- TOTAL: double (nullable = true)\n-- numbers: double (nullable = true)\n-- Score_B10: double (nullable = true)\n-- Risk_C: double (nullable = true)\n-- Money_Value: double (nullable = true)\n-- Score_MV: double (nullable = true)\n-- Risk_D: double (nullable = true)\n-- District_Loss: integer (nullable = true)\n-- PROB16: double (nullable = true)\n-- RiSk_E: double (nullable = true)\n-- History: integer (nullable = true)\n-- Prob19: double (nullable = true)\n-- Risk_F: double (nullable = true)\n-- Score: double (nullable = true)\n-- Inherent_Risk: double (nullable = true)\n-- CONTROL_RISK: double (nullable = true)\n-- Detection_Risk: double (nullable = true)\n-- Audit_Risk: double (nullable = true)\n-- Risk: integer (nullable = true)\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["df = df.drop(\"TOTAL\", \"LOCATION_ID\", \"Audit_Risk\", \"Detection_Risk\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Shape of Dataframe\nprint(\"There are {} rows and {} columns.\".format(df.count(), len(df.columns)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">There are 776 rows and 23 columns.\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["df = df.dropna()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Let's take a look at each feature's correlational value in relation to class\nnew_df = df.toPandas()\ncorr_matrix= new_df.corr()\nabs(corr_matrix[\"Risk\"].sort_values(ascending=True))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">13</span><span class=\"ansired\">]: </span>\nSector_score     0.393322\nPROB16           0.176818\nRisk_F           0.214423\nHistory          0.239354\nRisk_D           0.254253\nRisk_B           0.255181\nPARA_B           0.256920\nMoney_Value      0.256992\nProb19           0.298520\nnumbers          0.308017\nRisk_C           0.342006\nScore_B10        0.353664\nInherent_Risk    0.356881\nPARA_A           0.378547\nRisk_A           0.384869\nDistrict_Loss    0.403591\nRiSk_E           0.411594\nCONTROL_RISK     0.416285\nScore_A          0.619383\nScore_B6         0.635524\nScore_MV         0.688207\nScore            0.785813\nRisk             1.000000\nName: Risk, dtype: float64\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["df.createOrReplaceTempView(\"fraud\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["%sql\n\nselect Risk, count(0) from fraud group by Risk"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Risk</th><th>count(0)</th></tr></thead><tbody><tr><td>1</td><td>305</td></tr><tr><td>0</td><td>470</td></tr></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">15</span><span class=\"ansired\">]: </span>\n[&apos;Sector_score&apos;,\n &apos;PARA_A&apos;,\n &apos;Score_A&apos;,\n &apos;Risk_A&apos;,\n &apos;PARA_B&apos;,\n &apos;Score_B6&apos;,\n &apos;Risk_B&apos;,\n &apos;numbers&apos;,\n &apos;Score_B10&apos;,\n &apos;Risk_C&apos;,\n &apos;Money_Value&apos;,\n &apos;Score_MV&apos;,\n &apos;Risk_D&apos;,\n &apos;District_Loss&apos;,\n &apos;PROB16&apos;,\n &apos;RiSk_E&apos;,\n &apos;History&apos;,\n &apos;Prob19&apos;,\n &apos;Risk_F&apos;,\n &apos;Score&apos;,\n &apos;Inherent_Risk&apos;,\n &apos;CONTROL_RISK&apos;,\n &apos;Risk&apos;]\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nnew_df = df.withColumnRenamed(\"Risk\",\"label\")\n\nfeatures = ['Sector_score',\n 'PARA_A',\n 'Score_A',\n 'Risk_A',\n 'PARA_B',\n 'Score_B6',\n 'Risk_B',\n 'numbers',\n 'Score_B10',\n 'Risk_C',\n 'Money_Value',\n 'Score_MV',\n 'Risk_D',\n 'District_Loss',\n 'PROB16',\n 'RiSk_E',\n 'History',\n 'Prob19',\n 'Risk_F',\n 'Score',\n 'Inherent_Risk',\n 'CONTROL_RISK']\n\nassembler = VectorAssembler(\n    inputCols= features,\n    outputCol= \"features\")\noutput = assembler.transform(new_df)\ndisplay(output.select(\"features\", \"label\").head(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>features</th><th>label</th></tr></thead><tbody><tr><td>List(1, 22, List(), List(3.89, 4.18, 0.6, 2.508, 2.5, 0.2, 0.5, 5.0, 0.2, 1.0, 3.38, 0.2, 0.676, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 2.4, 8.574, 0.4))</td><td>1</td></tr><tr><td>List(1, 22, List(), List(3.89, 0.0, 0.2, 0.0, 4.83, 0.2, 0.966, 5.0, 0.2, 1.0, 0.94, 0.2, 0.188, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 2.0, 2.554, 0.4))</td><td>0</td></tr><tr><td>List(1, 22, List(), List(3.89, 0.51, 0.2, 0.102, 0.23, 0.2, 0.046, 5.0, 0.2, 1.0, 0.0, 0.2, 0.0, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 2.0, 1.548, 0.4))</td><td>0</td></tr><tr><td>List(1, 22, List(), List(3.89, 0.0, 0.2, 0.0, 10.8, 0.6, 6.48, 6.0, 0.6, 3.6, 11.75, 0.6, 7.05, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 4.4, 17.53, 0.4))</td><td>1</td></tr><tr><td>List(1, 22, List(), List(3.89, 0.0, 0.2, 0.0, 0.08, 0.2, 0.016, 5.0, 0.2, 1.0, 0.0, 0.2, 0.0, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 2.0, 1.416, 0.4))</td><td>0</td></tr><tr><td>List(1, 22, List(), List(3.89, 0.0, 0.2, 0.0, 0.83, 0.2, 0.166, 5.0, 0.2, 1.0, 2.95, 0.2, 0.59, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 2.0, 2.156, 0.4))</td><td>0</td></tr><tr><td>List(1, 22, List(), List(3.89, 1.1, 0.4, 0.44, 7.41, 0.4, 2.964, 5.0, 0.2, 1.0, 44.95, 0.6, 26.97, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 3.2, 31.774, 0.4))</td><td>1</td></tr><tr><td>List(1, 22, List(), List(3.89, 8.5, 0.6, 5.1, 12.03, 0.6, 7.218, 5.5, 0.4, 2.2, 7.79, 0.4, 3.116, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 4.2, 18.034, 0.4))</td><td>1</td></tr><tr><td>List(1, 22, List(), List(3.89, 8.4, 0.6, 5.04, 11.05, 0.6, 6.63, 5.5, 0.4, 2.2, 7.34, 0.4, 2.936, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 4.2, 17.206, 0.4))</td><td>1</td></tr><tr><td>List(1, 22, List(), List(3.89, 3.98, 0.6, 2.388, 0.99, 0.2, 0.198, 5.0, 0.2, 1.0, 1.93, 0.2, 0.386, 2.0, 0.2, 0.4, 0.0, 0.2, 0.0, 2.4, 4.372, 0.4))</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.feature import StandardScaler\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(output)\n\n# Normalize each feature to have unit standard deviation.\nscaledData = scalerModel.transform(output)\nscaledData.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+------+-------+------+------+--------+------+-------+---------+------+-----------+--------+------+-------------+------+------+-------+------+------+-----+-------------+------------+-----+--------------------+--------------------+\nSector_score|PARA_A|Score_A|Risk_A|PARA_B|Score_B6|Risk_B|numbers|Score_B10|Risk_C|Money_Value|Score_MV|Risk_D|District_Loss|PROB16|RiSk_E|History|Prob19|Risk_F|Score|Inherent_Risk|CONTROL_RISK|label|            features|      scaledFeatures|\n+------------+------+-------+------+------+--------+------+-------+---------+------+-----------+--------+------+-------------+------+------+-------+------+------+-----+-------------+------------+-----+--------------------+--------------------+\n        3.89|  4.18|    0.6| 2.508|   2.5|     0.2|   0.5|    5.0|      0.2|   1.0|       3.38|     0.2| 0.676|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.4|        8.574|         0.4|    1|[3.89,4.18,0.6,2....|[0.16007297027769...|\n        3.89|   0.0|    0.2|   0.0|  4.83|     0.2| 0.966|    5.0|      0.2|   1.0|       0.94|     0.2| 0.188|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.0|        2.554|         0.4|    0|[3.89,0.0,0.2,0.0...|[0.16007297027769...|\n        3.89|  0.51|    0.2| 0.102|  0.23|     0.2| 0.046|    5.0|      0.2|   1.0|        0.0|     0.2|   0.0|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.0|        1.548|         0.4|    0|[3.89,0.51,0.2,0....|[0.16007297027769...|\n        3.89|   0.0|    0.2|   0.0|  10.8|     0.6|  6.48|    6.0|      0.6|   3.6|      11.75|     0.6|  7.05|            2|   0.2|   0.4|      0|   0.2|   0.0|  4.4|        17.53|         0.4|    1|[3.89,0.0,0.2,0.0...|[0.16007297027769...|\n        3.89|   0.0|    0.2|   0.0|  0.08|     0.2| 0.016|    5.0|      0.2|   1.0|        0.0|     0.2|   0.0|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.0|        1.416|         0.4|    0|[3.89,0.0,0.2,0.0...|[0.16007297027769...|\n        3.89|   0.0|    0.2|   0.0|  0.83|     0.2| 0.166|    5.0|      0.2|   1.0|       2.95|     0.2|  0.59|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.0|        2.156|         0.4|    0|[3.89,0.0,0.2,0.0...|[0.16007297027769...|\n        3.89|   1.1|    0.4|  0.44|  7.41|     0.4| 2.964|    5.0|      0.2|   1.0|      44.95|     0.6| 26.97|            2|   0.2|   0.4|      0|   0.2|   0.0|  3.2|       31.774|         0.4|    1|[3.89,1.1,0.4,0.4...|[0.16007297027769...|\n        3.89|   8.5|    0.6|   5.1| 12.03|     0.6| 7.218|    5.5|      0.4|   2.2|       7.79|     0.4| 3.116|            2|   0.2|   0.4|      0|   0.2|   0.0|  4.2|       18.034|         0.4|    1|[3.89,8.5,0.6,5.1...|[0.16007297027769...|\n        3.89|   8.4|    0.6|  5.04| 11.05|     0.6|  6.63|    5.5|      0.4|   2.2|       7.34|     0.4| 2.936|            2|   0.2|   0.4|      0|   0.2|   0.0|  4.2|       17.206|         0.4|    1|[3.89,8.4,0.6,5.0...|[0.16007297027769...|\n        3.89|  3.98|    0.6| 2.388|  0.99|     0.2| 0.198|    5.0|      0.2|   1.0|       1.93|     0.2| 0.386|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.4|        4.372|         0.4|    0|[3.89,3.98,0.6,2....|[0.16007297027769...|\n        3.89|  5.43|    0.6| 3.258| 10.77|     0.6| 6.462|    5.0|      0.2|   1.0|       4.42|     0.2| 0.884|            2|   0.2|   0.4|      0|   0.2|   0.0|  3.6|       12.004|         0.4|    1|[3.89,5.43,0.6,3....|[0.16007297027769...|\n        3.89| 15.38|    0.6| 9.228| 40.14|     0.6|24.084|    5.0|      0.2|   1.0|       0.96|     0.2| 0.192|            2|   0.4|   0.8|      1|   0.4|   0.4|  4.0|       35.704|         1.2|    1|[3.89,15.38,0.6,9...|[0.16007297027769...|\n        3.89|  5.47|    0.6| 3.282|  7.63|     0.4| 3.052|    5.0|      0.2|   1.0|      10.43|     0.6| 6.258|            2|   0.2|   0.4|      1|   0.4|   0.4|  3.6|       14.392|         0.8|    1|[3.89,5.47,0.6,3....|[0.16007297027769...|\n        3.89|  1.09|    0.4| 0.436|  0.35|     0.2|  0.07|    5.0|      0.2|   1.0|        0.0|     0.2|   0.0|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.2|        1.906|         0.4|    0|[3.89,1.09,0.4,0....|[0.16007297027769...|\n        3.89|   0.0|    0.2|   0.0|  0.84|     0.2| 0.168|    5.0|      0.2|   1.0|      0.007|     0.2|0.0014|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.0|       1.5694|         0.4|    0|[3.89,0.0,0.2,0.0...|[0.16007297027769...|\n        3.89|  1.95|    0.4|  0.78|  9.01|     0.4| 3.604|    5.0|      0.2|   1.0|        9.0|     0.4|   3.6|            2|   0.2|   0.4|      0|   0.2|   0.0|  3.0|        9.384|         0.4|    1|[3.89,1.95,0.4,0....|[0.16007297027769...|\n        3.89|  8.54|    0.6| 5.124| 31.63|     0.6|18.978|    5.0|      0.2|   1.0|      41.28|     0.6|24.768|            2|   0.2|   0.4|      1|   0.4|   0.4|  4.2|        50.67|         0.8|    1|[3.89,8.54,0.6,5....|[0.16007297027769...|\n        3.89|  4.18|    0.6| 2.508|  4.83|     0.2| 0.966|    5.5|      0.4|   2.2|      14.03|     0.6| 8.418|            2|   0.2|   0.4|      0|   0.2|   0.0|  3.2|       14.492|         0.4|    1|[3.89,4.18,0.6,2....|[0.16007297027769...|\n        3.89|  1.81|    0.4| 0.724|  1.03|     0.2| 0.206|    5.0|      0.2|   1.0|        0.0|     0.2|   0.0|            2|   0.2|   0.4|      0|   0.2|   0.0|  2.2|         2.33|         0.4|    0|[3.89,1.81,0.4,0....|[0.16007297027769...|\n        3.89|  4.86|    0.6| 2.916| 46.78|     0.6|28.068|    5.5|      0.4|   2.2|      63.18|     0.6|37.908|            2|   0.2|   0.4|      0|   0.2|   0.0|  4.4|       71.492|         0.4|    1|[3.89,4.86,0.6,2....|[0.16007297027769...|\n+------------+------+-------+------+------+--------+------+-------+---------+------+-----------+--------+------+-------------+------+------+-------+------+------+-----+-------------+------------+-----+--------------------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["(train, test) = scaledData.randomSplit([0.8, 0.2], seed=0)\n\n# Cache the training and test datasets\ntrain.cache()\ntest.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">18</span><span class=\"ansired\">]: </span>DataFrame[Sector_score: double, PARA_A: double, Score_A: double, Risk_A: double, PARA_B: double, Score_B6: double, Risk_B: double, numbers: double, Score_B10: double, Risk_C: double, Money_Value: double, Score_MV: double, Risk_D: double, District_Loss: int, PROB16: double, RiSk_E: double, History: int, Prob19: double, Risk_F: double, Score: double, Inherent_Risk: double, CONTROL_RISK: double, label: int, features: vector, scaledFeatures: vector]\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(labelCol='label', featuresCol = 'scaledFeatures')\nevaluator = BinaryClassificationEvaluator()\n\n# Create the pipeline\npipeline = Pipeline(stages=[dtc])\n\n# Cross Validate the data and get the best model\nparamGrid = ParamGridBuilder().addGrid(dtc.maxBins, [65, 65, 65]).addGrid(dtc.maxDepth, [4, 6, 8]).build()\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\n# Use the best model for predictions\ncv_model = crossval.fit(train)\nbestModel = cv_model.bestModel\npredictions = bestModel.transform(test)\nerror= evaluator.evaluate(predictions)\nprint(str(error))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.9591836734693877\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorIndexer\n\n# Build the Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\")\n\n# Create the Pipeline\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(train)\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(train)\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model\nmodel = pipeline.fit(train)\n\n# Display Decision Tree\ndisplay(model.stages[-1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>treeNode</th></tr></thead><tbody><tr><td>{\"index\":9,\"featureType\":\"continuous\",\"prediction\":null,\"threshold\":5.015000000000001,\"categories\":null,\"feature\":20,\"overflow\":false}</td></tr><tr><td>{\"index\":7,\"featureType\":\"categorical\",\"prediction\":null,\"threshold\":null,\"categories\":[0.0,1.0],\"feature\":13,\"overflow\":false}</td></tr><tr><td>{\"index\":1,\"featureType\":\"categorical\",\"prediction\":null,\"threshold\":null,\"categories\":[0.0],\"feature\":13,\"overflow\":false}</td></tr><tr><td>{\"index\":0,\"featureType\":null,\"prediction\":0.0,\"threshold\":null,\"categories\":null,\"feature\":null,\"overflow\":false}</td></tr><tr><td>{\"index\":3,\"featureType\":\"continuous\",\"prediction\":null,\"threshold\":2.436,\"categories\":null,\"feature\":20,\"overflow\":false}</td></tr><tr><td>{\"index\":2,\"featureType\":null,\"prediction\":0.0,\"threshold\":null,\"categories\":null,\"feature\":null,\"overflow\":false}</td></tr><tr><td>{\"index\":5,\"featureType\":\"continuous\",\"prediction\":null,\"threshold\":4.5E-4,\"categories\":null,\"feature\":4,\"overflow\":false}</td></tr><tr><td>{\"index\":4,\"featureType\":null,\"prediction\":0.0,\"threshold\":null,\"categories\":null,\"feature\":null,\"overflow\":false}</td></tr><tr><td>{\"index\":6,\"featureType\":null,\"prediction\":1.0,\"threshold\":null,\"categories\":null,\"feature\":null,\"overflow\":false}</td></tr><tr><td>{\"index\":8,\"featureType\":null,\"prediction\":1.0,\"threshold\":null,\"categories\":null,\"feature\":null,\"overflow\":false}</td></tr><tr><td>{\"index\":10,\"featureType\":null,\"prediction\":1.0,\"threshold\":null,\"categories\":null,\"feature\":null,\"overflow\":false}</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer\n\n# Create a random forest pipeline\n\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\")\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n \n# Train model\nmodel = pipeline.fit(train)\n\n# Function to create feature importance dataframe\ndef ExtractFeatureImp(featureImp, dataset, featuresCol):\n    list_extract = []\n    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n    varlist = pd.DataFrame(list_extract)\n    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n    return(varlist.sort_values('score', ascending = False))\n\n# Store feature importance dataframe\nscores = ExtractFeatureImp(model.stages[-2].featureImportances, train, \"features\")\n\n# Create a table for SQL usage\nspark_df = sqlContext.createDataFrame(scores)\nspark_df.createOrReplaceTempView(\"scores\")\ndisplay(scores)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>idx</th><th>name</th><th>score</th></tr></thead><tbody><tr><td>20</td><td>Inherent_Risk</td><td>0.256762540063198</td></tr><tr><td>19</td><td>Score</td><td>0.11645304056542623</td></tr><tr><td>21</td><td>CONTROL_RISK</td><td>0.08156608441129279</td></tr><tr><td>12</td><td>Risk_D</td><td>0.06578918229155964</td></tr><tr><td>3</td><td>Risk_A</td><td>0.06553171506057519</td></tr><tr><td>10</td><td>Money_Value</td><td>0.06080688646399375</td></tr><tr><td>6</td><td>Risk_B</td><td>0.06056284291120133</td></tr><tr><td>2</td><td>Score_A</td><td>0.05940812195361865</td></tr><tr><td>1</td><td>PARA_A</td><td>0.058742304966833546</td></tr><tr><td>15</td><td>RiSk_E</td><td>0.05433943561631396</td></tr><tr><td>13</td><td>District_Loss</td><td>0.04600136881299684</td></tr><tr><td>11</td><td>Score_MV</td><td>0.04574355068949014</td></tr><tr><td>4</td><td>PARA_B</td><td>0.020315976785695997</td></tr><tr><td>5</td><td>Score_B6</td><td>0.004646898616700078</td></tr><tr><td>0</td><td>Sector_score</td><td>0.0021301443100251814</td></tr><tr><td>8</td><td>Score_B10</td><td>8.396724868677205E-4</td></tr><tr><td>18</td><td>Risk_F</td><td>3.602339942110537E-4</td></tr><tr><td>9</td><td>Risk_C</td><td>0.0</td></tr><tr><td>14</td><td>PROB16</td><td>0.0</td></tr><tr><td>7</td><td>numbers</td><td>0.0</td></tr><tr><td>16</td><td>History</td><td>0.0</td></tr><tr><td>17</td><td>Prob19</td><td>0.0</td></tr></tbody></table></div>"]}}],"execution_count":16}],"metadata":{"name":"Use Case India Audit","notebookId":2633113775396065},"nbformat":4,"nbformat_minor":0}
